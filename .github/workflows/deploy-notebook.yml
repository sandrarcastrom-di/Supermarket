name: Dynamic Databricks Notebook Deploy
on:
  push:
    branches:
      - main
jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v3
    
    - name: Install jq & curl
      run: sudo apt-get update && sudo apt-get install -y jq curl
    
    - name: Export multiple notebooks (raw)
      run: |
        ORIGIN_HOST=${{ secrets.DATABRICKS_ORIGIN_HOST }}
        ORIGIN_TOKEN=${{ secrets.DATABRICKS_ORIGIN_TOKEN }}
        NOTEBOOK_BASE="/Workspace/Users/sandrar.castrom@outlook.com/Orders_ETL"
        NOTEBOOKS=("1. Preparacion" "2.0 Ingest aisle data" "2.1 Ingest departments data" "2.2 Ingest products data" "2.3 Ingest order_products_prior data" "2.4 Ingest order_products_train data" "2.5 Ingest orders data" "3.0 Transform aisle" "3.1 Transform departments" "3.2 Transform products" "3.3 Transform order_products_prior" "3.4 Transform order_products_train" "3.5 Transform orders" "4.0 Load Dim Products" "4.1 Load Dim Customer" "4.2 Load Dim Date" "4.3 Load Dim Time" "4.4 Load Fact Orders")  # Agrega m√°s seg√∫n necesitesas

        mkdir -p notebooks_to_deploy

        for nb in "${NOTEBOOKS[@]}"; do
          echo "Exportando '$nb' en modo raw..."
          curl -sS -G \
            -H "Authorization: Bearer $ORIGIN_TOKEN" \
            --data-urlencode "path=$NOTEBOOK_BASE/$nb" \
            --data-urlencode "format=SOURCE" \
            --data-urlencode "direct_download=true" \
            "$ORIGIN_HOST/api/2.0/workspace/export" \
            -o "notebooks_to_deploy/${nb//\//_}.py"
        done 
    
    - name: Deploy notebooks to Destination Workspace
      run: |
        DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
        DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
        DEST_BASE="/py/scripts/main"
        for file in notebooks_to_deploy/*.py; do
          name=$(basename "$file" .py)
          dest_path="$DEST_BASE/$name"
          echo "Creando carpeta $DEST_BASE si no existe..."
          curl -s -X POST \
            -H "Authorization: Bearer $DEST_TOKEN" \
            -H "Content-Type: application/json" \
            -d "{\"path\":\"$DEST_BASE\"}" \
            "$DEST_HOST/api/2.0/workspace/mkdirs"
          echo "Importando $file ‚Üí $dest_path"
          response=$(curl -s -X POST \
            -H "Authorization: Bearer $DEST_TOKEN" \
            -H "Content-Type: multipart/form-data" \
            -F "path=$dest_path" \
            -F "format=SOURCE" \
            -F "language=PYTHON" \
            -F "overwrite=true" \
            -F "content=@$file" \
            "$DEST_HOST/api/2.0/workspace/import")
          echo "Response: $response"
        done
    
    - name: Check if workflow exists and delete if necessary
      run: |
        DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
        DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
        WORKFLOW_NAME="WF_ADB"
        
        echo "Verificando si existe el workflow: $WORKFLOW_NAME"
        
        # Listar todos los workflows y buscar por nombre
        workflows_response=$(curl -s -X GET \
          -H "Authorization: Bearer $DEST_TOKEN" \
          "$DEST_HOST/api/2.1/jobs/list")
        
        # Extraer job_id si existe el workflow
        existing_job_id=$(echo "$workflows_response" | jq -r --arg name "$WORKFLOW_NAME" '.jobs[]? | select(.settings.name == $name) | .job_id')
        
        if [ "$existing_job_id" != "" ] && [ "$existing_job_id" != "null" ]; then
          echo "Workflow encontrado con ID: $existing_job_id. Eliminando..."
          delete_response=$(curl -s -X POST \
            -H "Authorization: Bearer $DEST_TOKEN" \
            -H "Content-Type: application/json" \
            -d "{\"job_id\": $existing_job_id}" \
            "$DEST_HOST/api/2.1/jobs/delete")
          echo "Delete response: $delete_response"
        else
          echo "No se encontr√≥ workflow existente con nombre: $WORKFLOW_NAME"
        fi
    
    - name: Get existing cluster ID
      run: |
        DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
        DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
        CLUSTER_NAME="Cl_Final_Project"
        
        echo "Buscando cluster existente: $CLUSTER_NAME"
        
        # Obtener lista de clusters
        clusters_response=$(curl -s -X GET \
          -H "Authorization: Bearer $DEST_TOKEN" \
          "$DEST_HOST/api/2.0/clusters/list")
        
        echo "Clusters response: $clusters_response"
        
        # Extraer cluster_id del cluster especificado
        cluster_id=$(echo "$clusters_response" | jq -r --arg name "$CLUSTER_NAME" '.clusters[]? | select(.cluster_name == $name) | .cluster_id')
        
        if [ "$cluster_id" != "" ] && [ "$cluster_id" != "null" ]; then
          echo "‚úÖ Cluster encontrado: $CLUSTER_NAME con ID: $cluster_id"
          echo "CLUSTER_ID=$cluster_id" >> $GITHUB_ENV
        else
          echo "‚ùå No se encontr√≥ el cluster: $CLUSTER_NAME"
          echo "Clusters disponibles:"
          echo "$clusters_response" | jq -r '.clusters[]? | .cluster_name'
          exit 1
        fi
    
    - name: Create Databricks Workflow WF_ADB
      run: |
        DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
        DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
        DEST_BASE="/py/scripts/main"
        CLUSTER_ID="${{ env.CLUSTER_ID }}"
        
        echo "Creando workflow: WF_ADB con cluster existente ID: $CLUSTER_ID"
        
        # Crear el JSON del workflow usando cluster existente
        cat > workflow_config.json << EOF
        {
          "name": "WF_Supermarket",
          "format": "MULTI_TASK",
          "tasks": [
            {
              "task_key": "1_Preparacion",
              "description": "Ejecuta notebook 1. Preparacion con par√°metros",
              "notebook_task": {
                "notebook_path": "/py/1. Preparacion",
                "source": "WORKSPACE",
                "base_parameters": {
                  "storageName": "safinalprojectdbs"
                }
              },
              "existing_cluster_id": "$CLUSTER_ID",
              "timeout_seconds": 3600,
              "max_retries": 2
            },
            {
              "task_key": "2_0_Ingest_aisle_data",
              "description": "Ejecuta notebook 2.0 Ingest aisle data con par√°metros",
              "notebook_task": {
                "notebook_path": "/py/2.0 Ingest aisle data",
                "source": "WORKSPACE",
                "base_parameters": {
                  "storage_name": "safinalprojectdbs",
                  "container": "raw",
                  "catalogo": "catalog_dev",
                  "esquema": "bronze"
                }
              },
              "existing_cluster_id": "$CLUSTER_ID",
              "timeout_seconds": 3600,
              "max_retries": 2,
              "depends_on": [
                {
                  "task_key": "1_Preparacion"
                }
              ]
            },
            {
              "task_key": "2_1_Ingest_departments_data",
              "description": "Ejecuta notebook 2.1 Ingest departments data con par√°metros",
              "notebook_task": {
                "notebook_path": "/py/2.1 Ingest departments data",
                "source": "WORKSPACE",
                "base_parameters": {
                  "storage_name": "safinalprojectdbs",
                  "container": "raw",
                  "catalogo": "catalog_dev",
                  "esquema": "bronze"
                }
              },
              "existing_cluster_id": "$CLUSTER_ID",
              "timeout_seconds": 3600,
              "max_retries": 2,
              "depends_on": [
                {
                  "task_key": "1_Preparacion"
                }
              ]
            },
            {
              "task_key": "2_2_Ingest_products_data",
              "description": "Ejecuta notebook 2.2 Ingest products data con par√°metros",
              "notebook_task": {
                "notebook_path": "/py/2.2 Ingest products data",
                "source": "WORKSPACE",
                "base_parameters": {
                  "storage_name": "safinalprojectdbs",
                  "container": "raw",
                  "catalogo": "catalog_dev",
                  "esquema": "bronze"
                }
              },
              "existing_cluster_id": "$CLUSTER_ID",
              "timeout_seconds": 3600,
              "max_retries": 2,
              "depends_on": [
                {
                  "task_key": "1_Preparacion"
                }
              ]
            },
            {
              "task_key": "2_3_Ingest_order_products_prior_data",
              "description": "Ejecuta notebook 2.3 Ingest order_products_prior data con par√°metros",
              "notebook_task": {
                "notebook_path": "/py/2.3 Ingest order_products_prior data",
                "source": "WORKSPACE",
                "base_parameters": {
                  "storage_name": "safinalprojectdbs",
                  "container": "raw",
                  "catalogo": "catalog_dev",
                  "esquema": "bronze"
                }
              },
              "existing_cluster_id": "$CLUSTER_ID",
              "timeout_seconds": 3600,
              "max_retries": 2,
              "depends_on": [
                {
                  "task_key": "1_Preparacion"
                }
              ]
            },
            {
              "task_key": "2_4_Ingest_order_products_train_data",
              "description": "Ejecuta notebook 2.4 Ingest order_products_train data con par√°metros",
              "notebook_task": {
                "notebook_path": "/py/2.4 Ingest order_products_train data",
                "source": "WORKSPACE",
                "base_parameters": {
                  "storage_name": "safinalprojectdbs",
                  "container": "raw",
                  "catalogo": "catalog_dev",
                  "esquema": "bronze"
                }
              },
              "existing_cluster_id": "$CLUSTER_ID",
              "timeout_seconds": 3600,
              "max_retries": 2,
              "depends_on": [
                {
                  "task_key": "1_Preparacion"
                }
              ]
            },
            {
              "task_key": "2_5_Ingest_order_data",
              "description": "Ejecuta notebook 2.5 Ingest orders data con par√°metros",
              "notebook_task": {
                "notebook_path": "/py/2.5 Ingest orders data",
                "source": "WORKSPACE",
                "base_parameters": {
                  "storage_name": "safinalprojectdbs",
                  "container": "raw",
                  "catalogo": "catalog_dev",
                  "esquema": "bronze"
                }
              },
              "existing_cluster_id": "$CLUSTER_ID",
              "timeout_seconds": 3600,
              "max_retries": 2,
              "depends_on": [
                {
                  "task_key": "1_Preparacion"
                }
              ]
            },
            {
              "task_key": "3_0_Transform_aisle",
              "description": "Ejecuta notebook 3.0 Transform aisle con par√°metros",
              "notebook_task": {
                "notebook_path": "/py/3.0 Transform aisle",
                "source": "WORKSPACE",
                "base_parameters": {
                  "catalogo": "catalog_dev",
                  "esquema_source": "bronze",
                  "esquema_sink": "silver"
                }
              },
              "existing_cluster_id": "$CLUSTER_ID",
              "timeout_seconds": 3600,
              "max_retries": 2,
              "depends_on": [
                {
                  "task_key": "2_0_Ingest_aisle_data"
                }
              ]
            },
            {
              "task_key": "3_1_Transform_departments",
              "description": "Ejecuta notebook 3.1 Transform departments con par√°metros",
              "notebook_task": {
                "notebook_path": "/py/3.1 Transform departments",
                "source": "WORKSPACE",
                "base_parameters": {
                  "catalogo": "catalog_dev",
                  "esquema_source": "bronze",
                  "esquema_sink": "silver"
                }
              },
              "existing_cluster_id": "$CLUSTER_ID",
              "timeout_seconds": 3600,
              "max_retries": 2,
              "depends_on": [
                {
                  "task_key": "2_1_Ingest_departments_data"
                }
              ]
            },
            {
              "task_key": "3_2_Transform_products",
              "description": "Ejecuta notebook 3.2 Transform products con par√°metros",
              "notebook_task": {
                "notebook_path": "/py/3.2 Transform products",
                "source": "WORKSPACE",
                "base_parameters": {
                  "catalogo": "catalog_dev",
                  "esquema_source": "bronze",
                  "esquema_sink": "silver"
                }
              },
              "existing_cluster_id": "$CLUSTER_ID",
              "timeout_seconds": 3600,
              "max_retries": 2,
              "depends_on": [
                {
                  "task_key": "2_2_Ingest_products_data"
                }
              ]
            },
            {
              "task_key": "3_3_Transform_order_products_prior",
              "description": "Ejecuta notebook 3.3 Transform order_products_prior con par√°metros",
              "notebook_task": {
                "notebook_path": "/py/3.3 Transform order_products_prior",
                "source": "WORKSPACE",
                "base_parameters": {
                  "catalogo": "catalog_dev",
                  "esquema_source": "bronze",
                  "esquema_sink": "silver"
                }
              },
              "existing_cluster_id": "$CLUSTER_ID",
              "timeout_seconds": 3600,
              "max_retries": 2,
              "depends_on": [
                {
                  "task_key": "2_3_Ingest_order_products_prior_data"
                }
              ]
            },
            {
              "task_key": "3_4_Transform_order_products_train",
              "description": "Ejecuta notebook 3.4 Transform order_products_train con par√°metros",
              "notebook_task": {
                "notebook_path": "/py/3.4 Transform order_products_train",
                "source": "WORKSPACE",
                "base_parameters": {
                  "catalogo": "catalog_dev",
                  "esquema_source": "bronze",
                  "esquema_sink": "silver"
                }
              },
              "existing_cluster_id": "$CLUSTER_ID",
              "timeout_seconds": 3600,
              "max_retries": 2,
              "depends_on": [
                {
                  "task_key": "2_4_Ingest_order_products_train_data"
                }
              ]
            },
            {
              "task_key": "3_5_Transform_orders",
              "description": "Ejecuta notebook 3.5 Transform orders con par√°metros",
              "notebook_task": {
                "notebook_path": "/py/3.5 Transform orders",
                "source": "WORKSPACE",
                "base_parameters": {
                  "catalogo": "catalog_dev",
                  "esquema_source": "bronze",
                  "esquema_sink": "silver"
                }
              },
              "existing_cluster_id": "$CLUSTER_ID",
              "timeout_seconds": 3600,
              "max_retries": 2,
              "depends_on": [
                {
                  "task_key": "2_5_Ingest_order_data"
                }
              ]
            },
            {
              "task_key": "4_0_Dim_Products",
              "description": "Ejecuta notebook 4.0 Load Dim Products con par√°metros",
              "notebook_task": {
                "notebook_path": "/py/4.0 Load Dim Products",
                "source": "WORKSPACE",
                "base_parameters": {
                  "catalogo": "catalog_dev",
                  "esquema_source": "silver",
                  "esquema_sink": "golden"
                }
              },
              "existing_cluster_id": "$CLUSTER_ID",
              "timeout_seconds": 3600,
              "max_retries": 2,
              "depends_on": [
                {
                  "task_key": "3_0_Transform_aisle"
                },
                {
                  "task_key": "3_1_Transform_departments"
                },
                {
                  "task_key": "3_2_Transform_products"
                }
              ]
            },
            {
              "task_key": "4_1_Dim_Customer",
              "description": "Ejecuta notebook 4.1 Load Dim Customers con par√°metros",
              "notebook_task": {
                "notebook_path": "/py/4.1 Load Dim Customer",
                "source": "WORKSPACE",
                "base_parameters": {
                  "catalogo": "catalog_dev",
                  "esquema_source": "silver",
                  "esquema_sink": "golden"
                }
              },
              "existing_cluster_id": "$CLUSTER_ID",
              "timeout_seconds": 3600,
              "max_retries": 2,
              "depends_on": [
                {
                  "task_key": "3_5_Transform_orders"
                }
              ]
            },
            {
              "task_key": "4_2_Dim_Date",
              "description": "Ejecuta notebook 4.2 Load Dim Date con par√°metros",
              "notebook_task": {
                "notebook_path": "/py/4.2 Load Dim Date",
                "source": "WORKSPACE",
                "base_parameters": {
                  "catalogo": "catalog_dev",
                  "esquema_source": "silver",
                  "esquema_sink": "golden"
                }
              },
              "existing_cluster_id": "$CLUSTER_ID",
              "timeout_seconds": 3600,
              "max_retries": 2,
              "depends_on": [
                {
                  "task_key": "3_5_Transform_orders"
                }
              ]
            },
            {
              "task_key": "4_3_Dim_Time",
              "description": "Ejecuta notebook 4.3 Load Dim Time con par√°metros",
              "notebook_task": {
                "notebook_path": "/py/4.3 Load Dim Time",
                "source": "WORKSPACE",
                "base_parameters": {
                  "catalogo": "catalog_dev",
                  "esquema_source": "silver",
                  "esquema_sink": "golden"
                }
              },
              "existing_cluster_id": "$CLUSTER_ID",
              "timeout_seconds": 3600,
              "max_retries": 2,
              "depends_on": [
                {
                  "task_key": "3_5_Transform_orders"
                }
              ]
            },
            {
              "task_key": "4_4_Fact_Orders",
              "description": "Ejecuta notebook 4.4 Load Fact Orders con par√°metros",
              "notebook_task": {
                "notebook_path": "/py/4.4 Load Fact Orders",
                "source": "WORKSPACE",
                "base_parameters": {
                  "catalogo": "catalog_dev",
                  "esquema_source": "silver",
                  "esquema_sink": "golden"
                }
              },
              "existing_cluster_id": "$CLUSTER_ID",
              "timeout_seconds": 3600,
              "max_retries": 2,
              "depends_on": [
                {
                  "task_key": "3_3_Transform_order_products_prior"
                },
                {
                  "task_key": "3_4_Transform_order_products_train"
                },
                {
                  "task_key": "3_5_Transform_orders"
                }
              ]
            }
          ],
          "schedule": {
            "quartz_cron_expression": "0 0 8 * * ?",
            "timezone_id": "America/Lima",
            "pause_status": "PAUSED"
          },
          "email_notifications": {
            "on_failure": [],
            "on_success": [],
            "no_alert_for_skipped_runs": false
          },
          "webhook_notifications": {},
          "timeout_seconds": 7200,
          "max_concurrent_runs": 1,
          "tags": {
            "environment": "production",
            "created_by": "github_actions",
            "project": "automated_deployment",
            "cluster_used": "cluster_SD"
          }
        }
        EOF
        
        # Crear el workflow
        create_response=$(curl -s -X POST \
          -H "Authorization: Bearer $DEST_TOKEN" \
          -H "Content-Type: application/json" \
          -d @workflow_config.json \
          "$DEST_HOST/api/2.1/jobs/create")
        
        echo "Workflow creation response: $create_response"
        
        # Extraer job_id del response
        job_id=$(echo "$create_response" | jq -r '.job_id')
        
        if [ "$job_id" != "" ] && [ "$job_id" != "null" ]; then
          echo "‚úÖ Workflow 'WF_ADB' creado exitosamente con ID: $job_id"
          
          # Obtener detalles del workflow creado
          workflow_details=$(curl -s -X GET \
            -H "Authorization: Bearer $DEST_TOKEN" \
            "$DEST_HOST/api/2.1/jobs/get?job_id=$job_id")
          
          echo "Detalles del workflow:"
          echo "$workflow_details" | jq '.settings | {name, tasks: (.tasks | map({task_key, notebook_task: .notebook_task.notebook_path}))}'
        else
          echo "‚ùå Error al crear el workflow"
          echo "Response completo: $create_response"
          exit 1
        fi
    
    - name: Validate Workflow Configuration
      run: |
        DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
        DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
        WORKFLOW_NAME="WF_ADB"
        
        echo "üîç Validando la configuraci√≥n del workflow creado..."
        
        # Obtener lista de workflows y encontrar el reci√©n creado
        workflows_list=$(curl -s -X GET \
          -H "Authorization: Bearer $DEST_TOKEN" \
          "$DEST_HOST/api/2.1/jobs/list")
        
        job_id=$(echo "$workflows_list" | jq -r --arg name "$WORKFLOW_NAME" '.jobs[]? | select(.settings.name == $name) | .job_id')
        
        if [ "$job_id" != "" ] && [ "$job_id" != "null" ]; then
          echo "‚úÖ Workflow encontrado con ID: $job_id"
          
          # Obtener configuraci√≥n detallada
          job_details=$(curl -s -X GET \
            -H "Authorization: Bearer $DEST_TOKEN" \
            "$DEST_HOST/api/2.1/jobs/get?job_id=$job_id")
          
          echo "üìã Resumen del workflow:"
          echo "Nombre: $(echo "$job_details" | jq -r '.settings.name')"
          echo "N√∫mero de tareas: $(echo "$job_details" | jq '.settings.tasks | length')"
          echo ""
          echo "üìù Tareas configuradas:"
          echo "$job_details" | jq -r '.settings.tasks[] | "- " + .task_key + " ‚Üí " + .notebook_task.notebook_path'
          echo ""
          echo "üñ•Ô∏è  Cluster configurado:"
          echo "Cluster ID: $(echo "$job_details" | jq -r '.settings.tasks[0].existing_cluster_id')"
          echo "Cluster Name: Cl_Final_Project (reutilizado)"
          
        else
          echo "‚ùå No se pudo encontrar el workflow creado"
          exit 1
        fi
    
    - name: Execute Workflow WF_Supermarket
      run: |
        DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
        DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
        WORKFLOW_NAME="WF_Supermarket"
        
        echo "üöÄ Ejecutando workflow: $WORKFLOW_NAME"
        
        # Obtener job_id del workflow
        workflows_list=$(curl -s -X GET \
          -H "Authorization: Bearer $DEST_TOKEN" \
          "$DEST_HOST/api/2.1/jobs/list")
        
        job_id=$(echo "$workflows_list" | jq -r --arg name "$WORKFLOW_NAME" '.jobs[]? | select(.settings.name == $name) | .job_id')
        
        if [ "$job_id" != "" ] && [ "$job_id" != "null" ]; then
          echo "‚úÖ Workflow encontrado con ID: $job_id"
          
          # Ejecutar el workflow
          run_response=$(curl -s -X POST \
            -H "Authorization: Bearer $DEST_TOKEN" \
            -H "Content-Type: application/json" \
            -d "{\"job_id\": $job_id}" \
            "$DEST_HOST/api/2.1/jobs/run-now")
          
          run_id=$(echo "$run_response" | jq -r '.run_id')
          
          if [ "$run_id" != "" ] && [ "$run_id" != "null" ]; then
            echo "üéØ Workflow ejecutado exitosamente!"
            echo "Run ID: $run_id"
            echo "WORKFLOW_RUN_ID=$run_id" >> $GITHUB_ENV
            echo "WORKFLOW_JOB_ID=$job_id" >> $GITHUB_ENV
            
            # Mostrar URL del workflow en ejecuci√≥n
            echo "üîó URL del workflow: $DEST_HOST/jobs/$job_id/runs/$run_id"
            
          else
            echo "‚ùå Error al ejecutar el workflow"
            echo "Response: $run_response"
            exit 1
          fi
        else
          echo "‚ùå No se pudo encontrar el workflow para ejecutar"
          exit 1
        fi
    
    - name: Monitor Workflow Execution
      run: |
        DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
        DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
        RUN_ID="${{ env.WORKFLOW_RUN_ID }}"
        JOB_ID="${{ env.WORKFLOW_JOB_ID }}"
        
        echo "üìä Monitoreando ejecuci√≥n del workflow..."
        echo "Job ID: $JOB_ID"
        echo "Run ID: $RUN_ID"
        
        # Monitorear por m√°ximo 10 minutos (600 segundos)
        max_wait_time=600
        wait_time=0
        check_interval=30
        
        while [ $wait_time -lt $max_wait_time ]; do
          # Obtener estado actual
          run_status=$(curl -s -X GET \
            -H "Authorization: Bearer $DEST_TOKEN" \
            "$DEST_HOST/api/2.1/jobs/runs/get?run_id=$RUN_ID")
          
          state=$(echo "$run_status" | jq -r '.state.life_cycle_state')
          result_state=$(echo "$run_status" | jq -r '.state.result_state // "RUNNING"')
          
          echo "‚è±Ô∏è  Estado actual: $state ($result_state) - Tiempo transcurrido: ${wait_time}s"
          
          # Mostrar progreso de las tareas
          echo "$run_status" | jq -r '.tasks[]? | "  üìã " + .task_key + ": " + .state.life_cycle_state + " (" + (.state.result_state // "RUNNING") + ")"'
          
          case "$state" in
            "TERMINATED")
              if [ "$result_state" = "SUCCESS" ]; then
                echo "üéâ ¬°Workflow completado exitosamente!"
                
                # Mostrar resumen final
                echo ""
                echo "üìà Resumen de ejecuci√≥n:"
                echo "$run_status" | jq -r '.tasks[]? | "‚úÖ " + .task_key + " ‚Üí " + (.state.result_state // "SUCCESS")'
                
                # Obtener duraci√≥n
                start_time=$(echo "$run_status" | jq -r '.start_time')
                end_time=$(echo "$run_status" | jq -r '.end_time')
                if [ "$start_time" != "null" ] && [ "$end_time" != "null" ]; then
                  duration=$((($end_time - $start_time) / 1000))
                  echo "‚è∞ Duraci√≥n total: ${duration} segundos"
                fi
                
                exit 0
              else
                echo "‚ùå Workflow termin√≥ con errores: $result_state"
                echo "üìã Detalles de las tareas:"
                echo "$run_status" | jq -r '.tasks[]? | "‚ùå " + .task_key + ": " + (.state.result_state // "UNKNOWN")'
                exit 1
              fi
              ;;
            "INTERNAL_ERROR"|"SKIPPED")
              echo "‚ùå Workflow fall√≥ con estado: $state"
              exit 1
              ;;
            *)
              # Estados: PENDING, RUNNING, TERMINATING
              echo "‚è≥ Workflow a√∫n ejecut√°ndose..."
              ;;
          esac
          
          sleep $check_interval
          wait_time=$((wait_time + check_interval))
        done
        
        echo "‚ö†Ô∏è  Timeout: El workflow a√∫n se est√° ejecutando despu√©s de $max_wait_time segundos"
        echo "üîó Verifica el estado en: $DEST_HOST/jobs/$JOB_ID/runs/$RUN_ID"
        echo "‚ÑπÔ∏è  El workflow seguir√° ejecut√°ndose en Databricks"
        exit 0
    
    - name: Clean up
      run: |
        rm -rf notebooks_to_deploy
        rm -f workflow_config.json
    
    - name: Done
      run: |
        echo "üéâ ¬°Despliegue y ejecuci√≥n completados exitosamente!"
        echo ""
        echo "üìä Resumen:"
        echo "‚úÖ Notebooks desplegados: ntbk_1, ntbk_2"
        echo "‚úÖ Workflow creado: WF_ADB"
        echo "‚úÖ Tareas configuradas:"
        echo "   - tarea1_notebook1 (ntbk_1)"
        echo "   - tarea2_notebook2 (ntbk_2)"
        echo "‚úÖ Cluster existente: cluster_SD configurado"
        echo "üöÄ Workflow ejecutado autom√°ticamente"
        echo ""
        echo "üîó Accede a tu workspace de Databricks para ver los resultados detallados"